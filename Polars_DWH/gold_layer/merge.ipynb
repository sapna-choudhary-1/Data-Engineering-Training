{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b985cd15",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "770862e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "from helper_utils import log_step\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv(override=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e796d39",
   "metadata": {},
   "source": [
    "### MERGE HELPER FUNCTIONS\n",
    "- Provides functions to define how to: \n",
    "    - apply joins\n",
    "    - prepare the source\n",
    "    - assign surrogate_keys\n",
    "    - assign unknown_keys\n",
    "    - handle data-loading\n",
    "    - perform the  merge-operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49c3876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARS_DWH = Path(os.getenv(\"POLARS_DWH\"))\n",
    "PARQUET_FILES_DIR = Path(os.getenv(\"PARQUET_FILES_DIR\"))\n",
    "\n",
    "gold_parquet = PARQUET_FILES_DIR/'gold_layer'\n",
    "gold_parquet.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74ab6074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Utility: apply joins ------------------\n",
    "def apply_joins(base: pl.DataFrame, joins: list[tuple[pl.DataFrame, str | list[str], str]]) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply multiple joins on a base DataFrame.\n",
    "\n",
    "    Operations:\n",
    "    - Sequentially join with provided DataFrames using given keys and join types.\n",
    "    \"\"\"\n",
    "    for df, on, how in joins:\n",
    "        if df is None:\n",
    "            raise ValueError(f\"Tried joining with None on={on}, how={how}\")\n",
    "        base = base.join(df, on=on, how=how)\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "edb6d3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepare Source ------------------\n",
    "def prepare_src(\n",
    "    df_src: pl.DataFrame,\n",
    "    key_col: str,\n",
    "    attr_cols: Optional[list[str]],\n",
    "    gold_parquet: Path,\n",
    "    is_composite: bool,\n",
    "    df_src_initial: Optional[pl.DataFrame],\n",
    "    extra_cols: Optional[list[str]] = None,\n",
    ") -> tuple[pl.DataFrame, list[str], list[str]]:\n",
    "    \"\"\"\n",
    "    Prepare the source DataFrame:\n",
    "    - For simple dims: keep natural key col only.\n",
    "    - For composite dims: join surrogate keys from parent dims.\n",
    "    - For hierarchical dims: keep natural key col + join surrogate keys from parent dims.\n",
    "    - Optionally, if present, join extra columns back from the original staging.\n",
    "    \"\"\"\n",
    "    # log_step(\"INSIDE PREPARE_SRC\")\n",
    "    skey_cols, final_tbl_common_skeys = [], []\n",
    "\n",
    "    # --- Case 1: Simple dimension (single key only) ---\n",
    "    if attr_cols is None:\n",
    "        df_src = (\n",
    "            df_src.select(key_col)\n",
    "                  .drop_nulls()\n",
    "                  .unique(subset=[key_col])\n",
    "                  .filter(pl.col(key_col) != \"Unknown\")\n",
    "        )\n",
    "        skey_cols = [key_col]\n",
    "\n",
    "    # --- Case 2: Composite/ Hierarchical dimension (join with parent dims) ---\n",
    "    else:\n",
    "        for col in attr_cols:\n",
    "            dim_file = gold_parquet / f\"gold_dim_{col}.parquet\"\n",
    "            logger.info(f\"Looking for parent dim: {dim_file}\")\n",
    "\n",
    "            # Always keep base key for non-composite\n",
    "            if not is_composite:\n",
    "                skey_cols.append(key_col)\n",
    "                final_tbl_common_skeys.append(key_col)\n",
    "\n",
    "            if dim_file.exists():\n",
    "                # Use surrogate key from parent\n",
    "                skey = f\"{col}_skey\"\n",
    "                skey_cols.append(skey)\n",
    "                final_tbl_common_skeys.append(skey)\n",
    "\n",
    "                dim_df = pl.read_parquet(dim_file)\n",
    "\n",
    "                # always keep skey, conditionally keep col\n",
    "                cols = [skey] + ([col] if col in dim_df.columns else [])\n",
    "                dim_df = dim_df.select(cols)\n",
    "\n",
    "                # Add the skey col to the src tbl, to create foreign keys\n",
    "                join_key = col if col in df_src.columns else f\"{col}_skey\"\n",
    "                df_src = df_src.join(dim_df, on=join_key, how=\"left\")\n",
    "\n",
    "            else:\n",
    "                # Fallback if parent dim missing i.e. If no surrogate key exists yet → just use the raw column as natural key\n",
    "                skey_cols.append(col)\n",
    "\n",
    "        # Deduplicate and sort\n",
    "        skey_cols = list(dict.fromkeys(skey_cols))\n",
    "        final_tbl_common_skeys = list(dict.fromkeys(final_tbl_common_skeys))\n",
    "        df_src = df_src.select(skey_cols)\n",
    "\n",
    "        # Optional filter → only if the column exists\n",
    "        if not is_composite and df_src.schema.get(key_col) == pl.Utf8:\n",
    "            df_src = df_src.filter(pl.col(key_col) != \"Unknown\")\n",
    "\n",
    "        df_src = df_src.unique()\n",
    "\n",
    "    # --- Re-join extra columns if needed ---\n",
    "    if extra_cols:\n",
    "        if not is_composite:\n",
    "            join_cols = [key_col] + extra_cols\n",
    "            df_src = df_src.join(df_src_initial.select(join_cols), on=key_col, how=\"left\")\n",
    "        else:\n",
    "            join_cols = final_tbl_common_skeys + extra_cols\n",
    "            df_src = df_src.join(df_src_initial.select(join_cols), on=final_tbl_common_skeys, how=\"left\")\n",
    "\n",
    "    # log_step(\"AFTER PREPARE_SRC\", df_src)\n",
    "    return df_src, skey_cols, final_tbl_common_skeys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22aa5ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Assign Surrogate Keys ------------------\n",
    "def assign_surrogate_keys(\n",
    "    df: pl.DataFrame,\n",
    "    key_col: str,\n",
    "    start_offset: int = 1,\n",
    "    is_scd2: bool = True,\n",
    "    batch_id: int = 1\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Assign surrogate keys and metadata columns.\n",
    "\n",
    "    Operations:\n",
    "    - Add surrogate key column.\n",
    "    - Add SCD1 or SCD2 metadata fields.\n",
    "    \"\"\"\n",
    "    # log_step(\"INSIDE ASSIGN_SURROGATE\", df)\n",
    "    now = datetime.now().replace(microsecond=0)\n",
    "    assigned_key_col = key_col[:-3] if key_col.endswith(\"_id\") else key_col\n",
    "\n",
    "    # Surrogate key\n",
    "    df = df.with_row_index(f\"{assigned_key_col}_skey\", offset=start_offset)\n",
    "    df = df.with_columns(pl.col(f\"{assigned_key_col}_skey\").cast(pl.Int64))\n",
    "\n",
    "    # Metadata cols\n",
    "    if is_scd2:\n",
    "        df = df.with_columns([\n",
    "            pl.lit(1).alias(\"is_active\"),\n",
    "            pl.lit(now).alias(\"start_date\"),\n",
    "            pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "            pl.lit(1).alias(\"is_current\"),\n",
    "            pl.lit(batch_id).alias(\"batch_id\"),\n",
    "            pl.lit(now).alias(\"load_timestamp\"),\n",
    "        ])\n",
    "    else:\n",
    "        df = df.with_columns([\n",
    "            pl.lit(1).alias(\"is_active\"),\n",
    "            pl.lit(batch_id).alias(\"batch_id\"),\n",
    "            pl.lit(now).alias(\"load_timestamp\"),\n",
    "        ])\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa6655cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Add Unknown Row ------------------\n",
    "def add_unknown_row(\n",
    "    df_final: pl.DataFrame,\n",
    "    key_col: str,\n",
    "    is_scd2: bool = True,\n",
    "    batch_id: int = 1,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Add an 'Unknown' row for simple dims (not composites).\n",
    "\n",
    "    Operations:\n",
    "    - Generate next surrogate key.\n",
    "    - Populate with default values and metadata.\n",
    "    \"\"\"\n",
    "    now = datetime.now().replace(microsecond=0)\n",
    "    assigned_key_col = key_col[:-3] if key_col.endswith(\"_id\") else key_col\n",
    "    next_skey = int(df_final[f\"{assigned_key_col}_skey\"].max()) + 1 if not df_final.is_empty() else 1\n",
    "\n",
    "    base_data = {\n",
    "        f\"{assigned_key_col}_skey\": [next_skey],\n",
    "        key_col: [\"Unknown\"],\n",
    "        \"is_active\": [1],\n",
    "        \"batch_id\": [batch_id],\n",
    "        \"load_timestamp\": [now],\n",
    "    }\n",
    "\n",
    "    if is_scd2:\n",
    "        base_data.update({\n",
    "            \"start_date\": [now],\n",
    "            \"end_date\": [None],\n",
    "            \"is_current\": [1],\n",
    "        })\n",
    "\n",
    "    unknown_row = pl.DataFrame(base_data).cast(df_final.schema)\n",
    "    return unknown_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74675629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Initial Load ------------------\n",
    "def handle_initial_load(\n",
    "    df_src: pl.DataFrame,\n",
    "    key_col: str,\n",
    "    attr_cols: Optional[list[str]],\n",
    "    is_scd2: bool,\n",
    "    batch_id: int,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles the case when target table is empty (initial load).\n",
    "\n",
    "    Operations:\n",
    "    - Assign surrogate keys.\n",
    "    - Add 'Unknown' row if applicable.\n",
    "    \"\"\"\n",
    "    # log_step(\"INSIDE INITIAL LOAD\")\n",
    "\n",
    "    # 1. Assign surrogate keys\n",
    "    df_final = assign_surrogate_keys(\n",
    "        df_src, key_col, start_offset=1, is_scd2=is_scd2, batch_id=batch_id\n",
    "    )\n",
    "    # log_step(\"DF_FINAL after ASSIGN_SURROGATE\", df_final)\n",
    "\n",
    "    # 2. Add \"Unknown\" row (only for single-col/hierarchy dims, not composite)\n",
    "    if attr_cols is None:\n",
    "        unknown_row = add_unknown_row(df_final, key_col, is_scd2=is_scd2, batch_id=batch_id)\n",
    "        \n",
    "        # --- 3. Final concat ---\n",
    "        df_final = pl.concat([df_final, unknown_row], how=\"vertical\")\n",
    "        # log_step(\"Added UNKNOWN row\", df_final)\n",
    "\n",
    "    # --- 4. Return ---\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf9da84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Incremental Load ------------------\n",
    "def handle_incremental_load(\n",
    "    df_src: pl.DataFrame,\n",
    "    df_tgt: pl.DataFrame,\n",
    "    key_col: str,\n",
    "    attr_cols: Optional[list[str]],\n",
    "    extra_cols: Optional[list[str]],\n",
    "    is_scd2: bool,\n",
    "    final_tbl_common_skeys: list[str],\n",
    "    skey_cols: list[str],\n",
    "    assigned_key_col: str,\n",
    "    batch_id: int,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Handles incremental load (when target has existing data).\n",
    "    Flow: Expire old rows → Keep common rows → Add new rows → Preserve 'Unknown' row\n",
    "    \"\"\"\n",
    "    # log_step(\"INSIDE INCREMENTAL LOAD\")\n",
    "    now = datetime.now().replace(microsecond=0)\n",
    "\n",
    "    # ------ 1. ------ \n",
    "    # --- Category C-1: Rows exists only in target, not in source → expire them (not drop) - (set end_date, mark inactive) ---\n",
    "    if is_scd2:\n",
    "        # --- Use only the current active rows ---\n",
    "        active_rows = df_tgt.filter(pl.col(\"is_current\") == 1)\n",
    "        # --- Fetch old expired rows ---\n",
    "        old_expired_rows = df_tgt.filter(pl.col(\"is_current\") == 0)\n",
    "\n",
    "        # --- Fetch newly expired rows ---\n",
    "        # prevent Unknown from expiring if attr_cols is None\n",
    "        if attr_cols is None:\n",
    "            newly_expired = (\n",
    "                active_rows.filter(pl.col(key_col) != \"Unknown\")\n",
    "                .join(df_src, on=skey_cols, how=\"anti\")\n",
    "            )\n",
    "        else:\n",
    "            newly_expired = active_rows.join(df_src, on=skey_cols, how=\"anti\")\n",
    "\n",
    "        if not newly_expired.is_empty():\n",
    "            newly_expired = newly_expired.with_columns([\n",
    "                pl.lit(0).alias(\"is_current\"),\n",
    "                pl.lit(now).alias(\"end_date\"),\n",
    "            ])\n",
    "    else:\n",
    "        # SCD1: no expiration logic\n",
    "        active_rows = df_tgt\n",
    "        old_expired_rows = df_tgt.head(0)\n",
    "        newly_expired = df_tgt.head(0)\n",
    "\n",
    "    # --- Category C-2: Common rows - Rows exists in both source and target → keep them as is ---\n",
    "    if extra_cols:\n",
    "        common_rows = active_rows.join(df_src, on=final_tbl_common_skeys, how=\"inner\").select(active_rows.columns)\n",
    "    else:\n",
    "        common_rows = active_rows.join(df_src, on=skey_cols, how=\"inner\")\n",
    "\n",
    "    # --- Category C-3: New rows - Rows exists only in the source, not in target ---\n",
    "    new_rows = df_src.join(active_rows, on=skey_cols, how=\"anti\")\n",
    "\n",
    "    # If new rows exists: Assign surrogate keys and other cols to them\n",
    "    if not new_rows.is_empty():\n",
    "        start_offset = int(df_tgt[f\"{assigned_key_col}_skey\"].max()) + 1\n",
    "        new_rows = assign_surrogate_keys(new_rows, key_col, start_offset, is_scd2, batch_id)\n",
    "    else:\n",
    "        new_rows = df_tgt.head(0)\n",
    "\n",
    "    # --- 2. Preserve \"Unknown\" row ---\n",
    "    unknown_row = (\n",
    "        df_tgt.filter(pl.col(key_col) == \"Unknown\") if (attr_cols is None) else df_tgt.head(0)\n",
    "    )\n",
    "\n",
    "    # --- Schema alignment (including extra cols) ---\n",
    "    common_schema = df_tgt.schema\n",
    "    if extra_cols:\n",
    "        for col in extra_cols:\n",
    "            if col not in common_schema:\n",
    "                common_schema[col] = df_src.schema[col]\n",
    "\n",
    "    all_rows = [old_expired_rows, newly_expired, common_rows, new_rows, unknown_row]\n",
    "    all_rows = [row.cast(common_schema, strict=False) for row in all_rows]\n",
    "\n",
    "    # --- 3. Final concat ---\n",
    "    df_final = pl.concat(all_rows, how=\"vertical\")\n",
    "    \n",
    "    # --- 4. Return ---\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6df1a683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Main Merge Function ------------------\n",
    "def merge_fn(\n",
    "    df_src: pl.DataFrame,\n",
    "    df_tgt: Optional[pl.DataFrame],\n",
    "    key_col: str,\n",
    "    attr_cols: Optional[list[str]],\n",
    "    is_scd2: bool,\n",
    "    extra_cols: Optional[list[str]] = None,\n",
    "    batch_id: int = 1,\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Main merge function for initial and incremental loads.\n",
    "\n",
    "    Operations:\n",
    "    - Prepare source DataFrame.\n",
    "    - Run initial load if target empty, else incremental load.\n",
    "    \"\"\"\n",
    "\n",
    "    log_step(\"INSIDE MAIN\")\n",
    "    logger.info(f\"attr_cols : {attr_cols}\")\n",
    "    logger.info(f\"extra_cols : {extra_cols}\")\n",
    "    logger.info(f\"df_src.columns : {df_src.columns}\")\n",
    "    \n",
    "    # --- Prepare required variables ---\n",
    "    assigned_key_col = key_col[:-3] if key_col.endswith(\"_id\") else key_col\n",
    "    is_composite = True if (attr_cols and key_col not in df_src.columns) else False\n",
    "\n",
    "    # --- Prepare df_src ---\n",
    "    df_src, skey_cols, final_tbl_common_skeys = prepare_src(\n",
    "        df_src, key_col, attr_cols, gold_parquet,\n",
    "        is_composite,\n",
    "        df_src_initial=df_src,\n",
    "        extra_cols=extra_cols\n",
    "    )\n",
    "\n",
    "    # Case 1: Initial Load\n",
    "    if df_tgt is None or df_tgt.is_empty():\n",
    "        return handle_initial_load(df_src, key_col, attr_cols, is_scd2, batch_id)\n",
    "\n",
    "    # Case 2: Incremental Load\n",
    "    return handle_incremental_load(\n",
    "        df_src, df_tgt, key_col, attr_cols, extra_cols, is_scd2, \n",
    "        final_tbl_common_skeys, skey_cols, assigned_key_col, batch_id\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
