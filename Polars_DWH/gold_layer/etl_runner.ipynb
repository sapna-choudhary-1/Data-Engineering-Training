{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d58f774d",
   "metadata": {},
   "source": [
    "### Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87f4223c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import polars as pl\n",
    "from sqlalchemy.engine import Engine\n",
    "\n",
    "import config\n",
    "from helper_utils import get_batch_id, load_cached_parquet\n",
    "\n",
    "top_level = Path().resolve().parent\n",
    "sys.path.append(str(top_level))\n",
    "from db_utils import engine\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66351223",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./construct_merge.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77256273",
   "metadata": {},
   "source": [
    "### RUNNER FUNCTIONS\n",
    "- Provides functions to: build_source -> run_merge_operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00d50a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARS_DWH = Path(os.getenv(\"POLARS_DWH\"))\n",
    "PARQUET_FILES_DIR = Path(os.getenv(\"PARQUET_FILES_DIR\"))\n",
    "\n",
    "staging_parquet = PARQUET_FILES_DIR/'staging_layer'\n",
    "staging_parquet.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gold_parquet = PARQUET_FILES_DIR/'gold_layer'\n",
    "gold_parquet.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f02f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Construct Source Dataframe ------------------\n",
    "def build_source(tbl: str, staging_parquet: Path, gold_parquet: Path) -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    - Create the source DataFrame using the staging layerâ€™s table and the corresponding gold layer tables (with surrogate keys).\n",
    "    - Apply the necessary column-level transformations.\n",
    "    \"\"\"\n",
    "    props = config.TABLE_CONFIG[tbl]\n",
    "\n",
    "    # --- Prepare list of tuples of all the joining tables, with 'on' and 'how' specified as 'joins' ---\n",
    "    gold_parquet_joins = []\n",
    "    for parquet_file, cols, on, how in props[\"joins\"]:\n",
    "        df_gold = load_cached_parquet(gold_parquet, parquet_file, cols)\n",
    "        if df_gold is None:\n",
    "            raise ValueError(f\"Join source {parquet_file} returned None! Check path: {gold_parquet}/{parquet_file}\")\n",
    "        gold_parquet_joins.append((df_gold, on, how))\n",
    "\n",
    "    # --- Get optional transform ---\n",
    "    transform_fn_name = props.get(\"transform_fn\")\n",
    "    transform_fn = config.transform_fn_map.get(transform_fn_name, None)\n",
    "\n",
    "    # --- Load staging ---\n",
    "    df_staging = load_cached_parquet(staging_parquet, props[\"staging_file\"])\n",
    "\n",
    "    # --- Key column filtering ---\n",
    "    if df_staging.schema[props[\"key_col\"]] == pl.Utf8:\n",
    "        df_staging = df_staging.filter(pl.col(props[\"key_col\"]).is_not_null() & (pl.col(props[\"key_col\"]) != \"\")).unique(props[\"key_col\"])\n",
    "    else:\n",
    "        df_staging = df_staging.filter(pl.col(props[\"key_col\"]).is_not_null() & (pl.col(props[\"key_col\"]) > 0))\n",
    "\n",
    "    # --- Apply joins ---\n",
    "    df_final = apply_joins(df_staging, gold_parquet_joins)\n",
    "\n",
    "    # --- Apply transform ---\n",
    "    if transform_fn:\n",
    "        df_final = transform_fn(df_final)\n",
    "\n",
    "    # --- Final select & sort ---\n",
    "    df_final = df_final.select(props[\"select_cols\"])\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77384f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Prepare Merge Execution ------------------\n",
    "def run_merge(\n",
    "    df_src: pl.DataFrame, \n",
    "    tgt_tbl: str, \n",
    "    engine: Engine, \n",
    "    schema_name: str\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    - Retrieve and configure all required columns based on the config.\n",
    "    - Invoke the core 'run merge' function.\n",
    "    - Persist results in both the local gold layer (Parquet files) and the SQL Server database.\n",
    "    - Prepare source DataFrame.\n",
    "    - Run initial load if target empty, else incremental load.\n",
    "    \"\"\"\n",
    "    # --- Fetch required attributes ---\n",
    "    props = config.TABLE_PROPS.get(tgt_tbl, {})\n",
    "    is_scd2 = props.get(\"is_scd2\", False)\n",
    "    is_dim  = props.get(\"is_dim\", True)\n",
    "\n",
    "    file_path = f\"gold_dim_{tgt_tbl}.parquet\" if is_dim else f\"gold_fact_{tgt_tbl}.parquet\"\n",
    "    df_tgt = load_cached_parquet(gold_parquet, file_path)\n",
    "\n",
    "    batch_id = get_batch_id(df_tgt)\n",
    "\n",
    "    # --- Fetch & Prepare required columns and variables ---\n",
    "    attr_cols = config.parent_map.get(tgt_tbl, None)\n",
    "    extra_cols = config.extra_col_map.get(tgt_tbl, None)\n",
    "    key_col   = config.key_col_map.get(tgt_tbl, tgt_tbl)\n",
    "\n",
    "    assigned_key_col = key_col[:-3] if key_col.endswith(\"_id\") else key_col\n",
    "    is_composite = True if (attr_cols and key_col not in df_src.columns) else False\n",
    "    \n",
    "    # --- Logs ---\n",
    "    log_step(\"INSIDE MAIN\")\n",
    "    logger.info(f\"attr_cols : {attr_cols}\")\n",
    "    logger.info(f\"extra_cols : {extra_cols}\")\n",
    "    logger.info(f\"df_src.columns : {df_src.columns}\")\n",
    "\n",
    "    # --- Prepare df_src ---\n",
    "    df_src, skey_cols, final_tbl_common_skeys = prepare_src(\n",
    "        df_src, key_col, attr_cols, gold_parquet,\n",
    "        is_composite,\n",
    "        df_src_initial=df_src,\n",
    "        extra_cols=extra_cols\n",
    "    )\n",
    "\n",
    "    # Case 1: Initial Load\n",
    "    if df_tgt is None or df_tgt.is_empty():\n",
    "        df_final = handle_initial_load(df_src, key_col, attr_cols, is_scd2, batch_id)\n",
    "\n",
    "    # Case 2: Incremental Load\n",
    "    else:\n",
    "        df_final = handle_incremental_load(\n",
    "            df_src, df_tgt, key_col, attr_cols, extra_cols, is_scd2, \n",
    "            final_tbl_common_skeys, skey_cols, assigned_key_col, batch_id\n",
    "        )\n",
    "    logger.info(f\"df_final.columns : {df_final.columns}\")\n",
    "\n",
    "    # Save to parquet (local gold layer)\n",
    "    df_final.write_parquet(gold_parquet / file_path)\n",
    "    \n",
    "    # Save to SQL Server (gold schema in DB)\n",
    "    table_name = f\"{schema_name}.{tgt_tbl}\"\n",
    "    df_final.write_database(\n",
    "        table_name=table_name,\n",
    "        connection=engine,\n",
    "        if_table_exists=\"replace\",\n",
    "    )\n",
    "    logger.info(f\"Saved {tgt_tbl} into DB schema {schema_name}\")\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7e8327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------ Build and Merge All ------------------\n",
    "def build_and_merge_all(table_groups: dict) -> dict[str, pl.DataFrame]:\n",
    "    \"\"\"\n",
    "    - Identify tables from the config as initial or final.\n",
    "    - Call the respective build and merge functions with the appropriate parameters.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # --- INITIAL group ---\n",
    "    for src_table, tgt_tbls in table_groups.get(\"initial\", {}).items():\n",
    "        src_file = staging_parquet / f\"staging_{src_table}.parquet\"\n",
    "        df_src = pl.read_parquet(src_file)\n",
    "\n",
    "        for tgt_tbl in tgt_tbls:\n",
    "            logger.info(f\"WORKING FOR INITIAL TABLE: {tgt_tbl}\")\n",
    "            results[tgt_tbl] = run_merge(df_src, tgt_tbl, engine, schema_name='gold')\n",
    "\n",
    "    # --- Final group ---\n",
    "    df_src_map = {}\n",
    "\n",
    "    for tgt_tbl in table_groups.get(\"final\", []):\n",
    "        logger.info(f\"WORKING FOR FINAL TABLE: {tgt_tbl}\")\n",
    "\n",
    "        df_src_map[tgt_tbl] = build_source(tgt_tbl, staging_parquet, gold_parquet)\n",
    "        df_src = df_src_map[tgt_tbl]\n",
    "\n",
    "        results[tgt_tbl] = run_merge(df_src, tgt_tbl, engine, schema_name='gold')\n",
    "        \n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
