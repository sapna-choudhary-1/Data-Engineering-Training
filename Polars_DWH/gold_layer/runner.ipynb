{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87f4223c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import urllib\n",
    "import polars as pl\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "import config\n",
    "\n",
    "load_dotenv(override=True) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66351223",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ./helper_utils.ipynb\n",
    "%run ./merge.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77256273",
   "metadata": {},
   "source": [
    "#### RUNNER FUNCTIONS ##\n",
    "- Provides functions to: build_source -> run_merge_operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "00d50a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLARS_DWH = Path(os.getenv(\"POLARS_DWH\"))\n",
    "PARQUET_FILES_DIR = Path(os.getenv(\"PARQUET_FILES_DIR\"))\n",
    "\n",
    "staging_parquet = PARQUET_FILES_DIR/'staging_layer'\n",
    "staging_parquet.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "gold_parquet = PARQUET_FILES_DIR/'gold_layer'\n",
    "gold_parquet.mkdir(parents=True, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4e794962",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------------- ##\n",
    "### Setup the db-connection\n",
    "params = urllib.parse.quote_plus(\n",
    "    \"DRIVER={ODBC Driver 17 for SQL Server};\"\n",
    "    \"SERVER=associatetraining.database.windows.net,1433;\"\n",
    "    \"DATABASE=associatetraining;\"\n",
    "    \"UID=training;\"\n",
    "    \"PWD=dFyUT1#$rKIh26;\"\n",
    ")\n",
    "\n",
    "engine = create_engine(f\"mssql+pyodbc:///?odbc_connect={params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f02f38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------------- ##\n",
    "def build_source(tbl: str, staging_parquet: Path, gold_parquet: Path) -> pl.DataFrame:\n",
    "    props = config.TABLE_CONFIG[tbl]\n",
    "\n",
    "    # --- Prepare list of tuples of all the joining tables, with 'on' and 'how' specified as 'joins' ---\n",
    "    gold_parquet_joins = []\n",
    "    for parquet_file, cols, on, how in props[\"joins\"]:\n",
    "        df_gold = load_cached_parquet(gold_parquet, parquet_file, cols)\n",
    "        if df_gold is None:\n",
    "            raise ValueError(f\"Join source {parquet_file} returned None! Check path: {gold_parquet}/{parquet_file}\")\n",
    "        gold_parquet_joins.append((df_gold, on, how))\n",
    "\n",
    "    # --- Get optional transform ---\n",
    "    transform_fn_name = props.get(\"transform_fn\")\n",
    "    transform_fn = config.transform_fn_map.get(transform_fn_name, None)\n",
    "\n",
    "    # --- Load staging ---\n",
    "    df_staging = load_cached_parquet(staging_parquet, props[\"staging_file\"])\n",
    "\n",
    "    # --- Key column filtering ---\n",
    "    if df_staging.schema[props[\"key_col\"]] == pl.Utf8:\n",
    "        df_staging = df_staging.filter(pl.col(props[\"key_col\"]).is_not_null() & (pl.col(props[\"key_col\"]) != \"\")).unique(props[\"key_col\"])\n",
    "    else:\n",
    "        df_staging = df_staging.filter(pl.col(props[\"key_col\"]).is_not_null() & (pl.col(props[\"key_col\"]) > 0))\n",
    "\n",
    "    # --- Apply joins ---\n",
    "    df_final = apply_joins(df_staging, gold_parquet_joins)\n",
    "\n",
    "    # --- Apply transform ---\n",
    "    if transform_fn:\n",
    "        df_final = transform_fn(df_final)\n",
    "\n",
    "    # --- Final select & sort ---\n",
    "    df_final = df_final.select(props[\"select_cols\"])\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77384f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------------- ##\n",
    "def run_merge(df_src: pl.DataFrame, tgt_tbl: str, engine, schema_name: str) -> pl.DataFrame:\n",
    "    props = config.TABLE_PROPS.get(tgt_tbl, {})\n",
    "    is_scd2 = props.get(\"is_scd2\", False)\n",
    "    is_dim  = props.get(\"is_dim\", True)\n",
    "\n",
    "    file_path = f\"gold_dim_{tgt_tbl}.parquet\" if is_dim else f\"gold_fact_{tgt_tbl}.parquet\"\n",
    "    df_tgt = load_cached_parquet(gold_parquet, file_path)\n",
    "    batch_id = get_batch_id(df_tgt)\n",
    "\n",
    "    attr_cols = config.parent_map.get(tgt_tbl, None)\n",
    "    extra_cols = config.extra_col_map.get(tgt_tbl, None)\n",
    "    key_col   = config.key_col_map.get(tgt_tbl, tgt_tbl)\n",
    "\n",
    "    df_final = merge_fn(\n",
    "        df_src=df_src,\n",
    "        df_tgt=df_tgt,\n",
    "        key_col=key_col,\n",
    "        attr_cols=attr_cols,\n",
    "        extra_cols=extra_cols,\n",
    "        is_scd2=is_scd2,\n",
    "        batch_id=batch_id\n",
    "    )\n",
    "    print(f\"df_final.columns : {df_final.columns}\")\n",
    "\n",
    "    # Save to parquet (local gold layer)\n",
    "    df_final.write_parquet(gold_parquet / file_path)\n",
    "    \n",
    "    # Save to SQL Server (gold schema in DB)\n",
    "    table_name = f\"{schema_name}.{tgt_tbl}\"\n",
    "    df_final.write_database(\n",
    "        table_name=table_name,\n",
    "        connection=engine,\n",
    "        if_table_exists=\"replace\",\n",
    "    )\n",
    "    print(f\"Saved {tgt_tbl} into DB schema {schema_name}\")\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7e8327c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## --------------------------------------------------- ##\n",
    "def build_and_merge_all(table_groups: dict) -> dict[str, pl.DataFrame]:\n",
    "    results = {}\n",
    "\n",
    "    # --- INITIAL group ---\n",
    "    for src_table, tgt_tbls in table_groups.get(\"initial\", {}).items():\n",
    "        src_file = staging_parquet / f\"staging_{src_table}.parquet\"\n",
    "        df_src = pl.read_parquet(src_file)\n",
    "\n",
    "        for tgt_tbl in tgt_tbls:\n",
    "            print(f\"\\n==============================================================================\\n\")\n",
    "            print(f\"WORKING FOR INITIAL TABLE: {tgt_tbl}\")\n",
    "            results[tgt_tbl] = run_merge(df_src, tgt_tbl, engine, schema_name='gold')\n",
    "\n",
    "    # --- Final group ---\n",
    "    df_src_map = {}\n",
    "\n",
    "    for tgt_tbl in table_groups.get(\"final\", []):\n",
    "        print(f\"\\n==============================================================================\\n\")\n",
    "        print(f\"WORKING FOR FINAL TABLE: {tgt_tbl}\")\n",
    "\n",
    "        df_src_map[tgt_tbl] = build_source(tgt_tbl, staging_parquet, gold_parquet)\n",
    "        df_src = df_src_map[tgt_tbl]\n",
    "\n",
    "        results[tgt_tbl] = run_merge(df_src, tgt_tbl, engine, schema_name='gold')\n",
    "        \n",
    "    return results\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
