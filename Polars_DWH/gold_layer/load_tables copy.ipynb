{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "163d61bb",
   "metadata": {},
   "source": [
    "# GOLD LAYER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c8ddacae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import date, timedelta, datetime\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "sys.path.append(os.path.abspath(\"..\"))\n",
    "import udfs as udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c30f9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sapna.choudhary/Data-Engineering-Training/Polars_DWH\n",
      "/home/sapna.choudhary/Data-Engineering-Training/Polars_DWH/staging_layer\n",
      "/home/sapna.choudhary/Data-Engineering-Training/Polars_DWH/gold_layer\n"
     ]
    }
   ],
   "source": [
    "load_dotenv(override=True) \n",
    "\n",
    "POLARS_DWH = Path(os.getenv(\"POLARS_DWH\"))\n",
    "print(POLARS_DWH)\n",
    "\n",
    "staging_dir = POLARS_DWH/'staging_layer'\n",
    "gold_dir = POLARS_DWH/'gold_layer'\n",
    "print(staging_dir, gold_dir, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79ff4eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dim_date(start_date: str = \"1954-08-18\", end_date: str = \"2025-07-30\") -> pl.DataFrame:\n",
    "    # generate date range\n",
    "    start = datetime.strptime(start_date, \"%Y-%m-%d\").date()\n",
    "    end = datetime.strptime(end_date, \"%Y-%m-%d\").date()\n",
    "    \n",
    "    num_days = (end - start).days + 1\n",
    "    date_list = [start + timedelta(days=i) for i in range(num_days)]\n",
    "    \n",
    "    df = pl.DataFrame({\"date\": date_list})\n",
    "    \n",
    "    df = (\n",
    "        df.with_columns([\n",
    "            pl.col(\"date\").dt.year().alias(\"year\"),\n",
    "            pl.col(\"date\").dt.month().alias(\"month\"),\n",
    "            pl.col(\"date\").dt.day().alias(\"day\"),\n",
    "            pl.col(\"date\").dt.weekday().alias(\"weekday\"),   # 0=Mon, 6=Sun\n",
    "            pl.col(\"date\").dt.strftime(\"%A\").alias(\"weekday_name\"),\n",
    "            pl.when(pl.col(\"date\").dt.weekday().is_in([5, 6]))\n",
    "              .then(pl.lit(1)).otherwise(pl.lit(0))\n",
    "              .alias(\"is_weekend\"),\n",
    "            (\"Q\" + pl.col(\"date\").dt.quarter().cast(pl.Utf8)).alias(\"fiscal_quarter\")\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # enforce types for base df\n",
    "    df = df.cast({\n",
    "        \"date\": pl.Date,\n",
    "        \"year\": pl.Int32,\n",
    "        \"month\": pl.Int32,\n",
    "        \"day\": pl.Int32,\n",
    "        \"weekday\": pl.Int32,\n",
    "        \"weekday_name\": pl.Utf8,\n",
    "        \"is_weekend\": pl.Int32,\n",
    "        \"fiscal_quarter\": pl.Utf8,\n",
    "    })\n",
    "    \n",
    "    # add special mapping row with same dtypes\n",
    "    df_unknown = pl.DataFrame({\n",
    "        \"date\": [date(1900,1,1)],\n",
    "        \"year\": [0],\n",
    "        \"month\": [0],\n",
    "        \"day\": [0],\n",
    "        \"weekday\": [0],\n",
    "        \"weekday_name\": [\"Unknown\"],\n",
    "        \"is_weekend\": [0],\n",
    "        \"fiscal_quarter\": [\"Q0\"]\n",
    "    }).cast(df.schema) \n",
    "    \n",
    "    df_final = pl.concat([df, df_unknown], how=\"vertical\")\n",
    "    \n",
    "    return df_final.sort(\"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "bb2dfd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_upsert_dim(\n",
    "    df_src: pl.DataFrame,\n",
    "    df_tgt: pl.DataFrame | None,\n",
    "    key_col: str,\n",
    "    batch_id: int,\n",
    "    keep_unknown: bool = False,\n",
    "    unknown_value: str = \"Unknown\"\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    Generalized UPSERT (MERGE) for dimension tables in Polars.\n",
    "\n",
    "    Rules:\n",
    "      - If key exists in both → keep\n",
    "      - If key exists only in source → insert\n",
    "      - If key exists only in target → delete (unless keep_unknown=True)\n",
    "      - If keep_unknown=True → always ensure an 'Unknown' row exists\n",
    "    \"\"\"\n",
    "    # --- 1. Clean + distinct source\n",
    "    df_src = df_src.select(key_col).drop_nulls().unique(subset=[key_col])\n",
    "    # print(\"\\n============\\ndf_src : \", df_src)\n",
    "\n",
    "    # --- 2. Handle first load (target missing)\n",
    "    if df_tgt is None or df_tgt.is_empty():\n",
    "        # filter out Unknown if flagged\n",
    "        base = df_src.filter(pl.col(key_col) != unknown_value) if keep_unknown else df_src\n",
    "        # print(\"\\n============\\nbase : \", base)\n",
    "\n",
    "        df_final = (\n",
    "            base\n",
    "            .with_row_index(f\"{key_col}_skey\", offset=1)\n",
    "            .with_columns([\n",
    "                pl.col(f\"{key_col}_skey\").cast(pl.Int64),\n",
    "                pl.lit(1).alias(\"is_active\"),\n",
    "                pl.lit(batch_id).alias(\"batch_id\"),\n",
    "                pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "            ])\n",
    "        )\n",
    "        # print(\"\\n============\\ndf_final : \", df_final)\n",
    "\n",
    "        # append Unknown row if required\n",
    "        if keep_unknown:\n",
    "            next_skey = int(df_final[f\"{key_col}_skey\"].max()) + 1 if not df_final.is_empty() else 1\n",
    "            unknown_row = pl.DataFrame({\n",
    "                # f\"{key_col}_skey\": [next_skey],\n",
    "                f\"{key_col}_skey\": pl.Series([next_skey], dtype=pl.Int64),\n",
    "                key_col: [unknown_value],\n",
    "                \"is_active\": [1],\n",
    "                \"batch_id\": [batch_id],\n",
    "                \"load_timestamp\": [datetime.now()],\n",
    "            })\n",
    "            unknown_row = unknown_row.cast(df_final.schema)\n",
    "            # print(\"\\n============\\nunknown_row : \", unknown_row)\n",
    "\n",
    "            df_final = pl.concat([df_final, unknown_row], how=\"vertical\")\n",
    "        \n",
    "            # print(\"\\n============\\ndf_final : \", df_final)\n",
    "        return df_final.sort(f\"{key_col}_skey\")\n",
    "\n",
    "    # --- 3. Incremental merge (target already exists) ---\n",
    "\n",
    "    # INSERT: new rows\n",
    "    new_rows = df_src.filter(~pl.col(key_col).is_in(df_tgt.select(key_col)[key_col].implode()))\n",
    "    if keep_unknown:\n",
    "        new_rows = new_rows.filter(pl.col(key_col) != unknown_value)\n",
    "        # print(\"\\n============\\ndf_src : \", df_src)\n",
    "\n",
    "    if not new_rows.is_empty():\n",
    "        start_offset = int(df_tgt[f\"{key_col}_skey\"].max()) + 1\n",
    "        new_rows = (\n",
    "            new_rows\n",
    "            .with_row_index(f\"{key_col}_skey\", offset=start_offset)\n",
    "            .with_columns([\n",
    "                pl.col(f\"{key_col}_skey\").cast(pl.Int64),\n",
    "                pl.lit(1).alias(\"is_active\"),\n",
    "                pl.lit(batch_id).alias(\"batch_id\"),\n",
    "                pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "            ])\n",
    "        )\n",
    "        # print(\"\\n============\\nnew_rows : \", new_rows)\n",
    "    else:\n",
    "        new_rows = df_tgt.head(0)\n",
    "        # print(\"\\n============\\nnew_rows : \", new_rows)\n",
    "\n",
    "    # KEEP: common keys\n",
    "    # common = df_tgt.filter(pl.col(key_col).is_in(df_src[key_col]))\n",
    "    common = df_tgt.filter(\n",
    "        pl.col(key_col).is_in(df_src.select(key_col)[key_col].implode())\n",
    "    )\n",
    "\n",
    "    # KEEP Unknown row (if flagged)\n",
    "    keep_unknown_row = df_tgt.filter(pl.col(key_col) == unknown_value) if keep_unknown else df_tgt.head(0)\n",
    "\n",
    "    # Final merge result\n",
    "    df_final = pl.concat([common, new_rows, keep_unknown_row], how=\"vertical\") \\\n",
    "                 .unique(subset=[key_col], keep=\"first\") \\\n",
    "                 .sort(f\"{key_col}_skey\")\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e8e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender False\n",
      "shape: (4, 5)\n",
      "┌─────────────┬─────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ gender_skey ┆ gender  ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---         ┆ ---     ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64         ┆ str     ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞═════════════╪═════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1           ┆ Other   ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.984558 │\n",
      "│ 2           ┆ Male    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.984558 │\n",
      "│ 4           ┆ Unknown ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.984558 │\n",
      "│ 5           ┆ Female  ┆ 1         ┆ 1        ┆ 2025-09-02 16:17:35.144812 │\n",
      "└─────────────┴─────────┴───────────┴──────────┴────────────────────────────┘\n",
      "marital_status False\n",
      "shape: (3, 5)\n",
      "┌─────────────────────┬────────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ marital_status_skey ┆ marital_status ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---                 ┆ ---            ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64                 ┆ str            ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞═════════════════════╪════════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1                   ┆ Unknown        ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.988122 │\n",
      "│ 2                   ┆ Married        ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.988122 │\n",
      "│ 3                   ┆ Single         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.988122 │\n",
      "└─────────────────────┴────────────────┴───────────┴──────────┴────────────────────────────┘\n",
      "customer_type False\n",
      "shape: (3, 5)\n",
      "┌────────────────────┬───────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ customer_type_skey ┆ customer_type ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---                ┆ ---           ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64                ┆ str           ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞════════════════════╪═══════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1                  ┆ Unknown       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.993054 │\n",
      "│ 2                  ┆ Prime         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.993054 │\n",
      "│ 3                  ┆ Non-prime     ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.993054 │\n",
      "└────────────────────┴───────────────┴───────────┴──────────┴────────────────────────────┘\n",
      "account_status False\n",
      "shape: (4, 5)\n",
      "┌─────────────────────┬────────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ account_status_skey ┆ account_status ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---                 ┆ ---            ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64                 ┆ str            ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞═════════════════════╪════════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1                   ┆ Suspended      ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.997322 │\n",
      "│ 2                   ┆ Active         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.997322 │\n",
      "│ 3                   ┆ Unknown        ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.997322 │\n",
      "│ 4                   ┆ Inactive       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.997322 │\n",
      "└─────────────────────┴────────────────┴───────────┴──────────┴────────────────────────────┘\n",
      "region True\n",
      "shape: (6, 5)\n",
      "┌─────────────┬───────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ region_skey ┆ region        ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---         ┆ ---           ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64         ┆ str           ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞═════════════╪═══════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1           ┆ North America ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.003568 │\n",
      "│ 2           ┆ Europe        ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.003568 │\n",
      "│ 3           ┆ Asia          ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.003568 │\n",
      "│ 4           ┆ South America ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.003568 │\n",
      "│ 6           ┆ Unknown       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.003964 │\n",
      "│ 7           ┆ Africa        ┆ 1         ┆ 1        ┆ 2025-09-02 16:17:35.176601 │\n",
      "└─────────────┴───────────────┴───────────┴──────────┴────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "table_name = 'customer'\n",
    "key_cols = {\"gender\": False, \"marital_status\": False, \"customer_type\": False, \"account_status\": False, \"region\": True}\n",
    "\n",
    "for col, keep_unknown in key_cols.items():\n",
    "    print(\"\\n--- Running merge for:\", col, \"keep_unknown:\", keep_unknown)\n",
    "\n",
    "    src_file_name = staging_dir / f\"staging_{table_name}.parquet\"\n",
    "    df_src = pl.read_parquet(src_file_name)\n",
    "\n",
    "    # load gold table if exists\n",
    "    tgt_file_name = gold_dir / f\"gold_dim_{col}.parquet\"\n",
    "    df_tgt = pl.read_parquet(tgt_file_name) if os.path.exists(tgt_file_name) else None\n",
    "\n",
    "    # ##### TEST FILTER #####\n",
    "    # if col == 'gender':\n",
    "    #     df_src = df_src.filter(pl.col(col) != 'Female')\n",
    "    #     print(df_src['gender'].unique())\n",
    "    # if col == 'region':\n",
    "    #     df_src = df_src.filter(pl.col(col) != 'Africa')\n",
    "    #     print(df_src['region'].unique())\n",
    "\n",
    "    # merge\n",
    "    df_tgt = merge_upsert_dim(df_src, df_tgt, col, batch_id=1, keep_unknown=keep_unknown)\n",
    "    print(df_tgt)\n",
    "\n",
    "    # save\n",
    "    df_tgt.write_parquet(tgt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e198b4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>marital_status_skey</th><th>marital_status</th><th>is_active</th><th>batch_id</th><th>load_timestamp</th></tr><tr><td>i64</td><td>str</td><td>i32</td><td>i32</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>1</td><td>&quot;Unknown&quot;</td><td>1</td><td>1</td><td>2025-09-02 16:12:50.988122</td></tr><tr><td>2</td><td>&quot;Married&quot;</td><td>1</td><td>1</td><td>2025-09-02 16:12:50.988122</td></tr><tr><td>3</td><td>&quot;Single&quot;</td><td>1</td><td>1</td><td>2025-09-02 16:12:50.988122</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 5)\n",
       "┌─────────────────────┬────────────────┬───────────┬──────────┬────────────────────────────┐\n",
       "│ marital_status_skey ┆ marital_status ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
       "│ ---                 ┆ ---            ┆ ---       ┆ ---      ┆ ---                        │\n",
       "│ i64                 ┆ str            ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
       "╞═════════════════════╪════════════════╪═══════════╪══════════╪════════════════════════════╡\n",
       "│ 1                   ┆ Unknown        ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.988122 │\n",
       "│ 2                   ┆ Married        ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.988122 │\n",
       "│ 3                   ┆ Single         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:50.988122 │\n",
       "└─────────────────────┴────────────────┴───────────┴──────────┴────────────────────────────┘"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dim_marital_status_parquet = pl.read_parquet(\"gold_dim_marital_status.parquet\")\n",
    "df_dim_marital_status_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "63f7308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "brand_tier False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (4, 5)\n",
      "┌─────────────────┬────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ brand_tier_skey ┆ brand_tier ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---             ┆ ---        ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64             ┆ str        ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞═════════════════╪════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1               ┆ Unknown    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.088061 │\n",
      "│ 2               ┆ Standard   ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.088061 │\n",
      "│ 3               ┆ Economy    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.088061 │\n",
      "│ 4               ┆ Premium    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.088061 │\n",
      "└─────────────────┴────────────┴───────────┴──────────┴────────────────────────────┘\n",
      "brand_name False\n",
      "shape: (14, 5)\n",
      "┌─────────────────┬────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ brand_name_skey ┆ brand_name ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---             ┆ ---        ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64             ┆ str        ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞═════════════════╪════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1               ┆ Lenovo     ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 2               ┆ Samsung    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 3               ┆ Puma       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 4               ┆ Hp         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 5               ┆ Oneplus    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ …               ┆ …          ┆ …         ┆ …        ┆ …                          │\n",
      "│ 10              ┆ Apple      ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 11              ┆ Sony       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 12              ┆ Unknown    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 13              ┆ Adidas     ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "│ 14              ┆ Dell       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.167035 │\n",
      "└─────────────────┴────────────┴───────────┴──────────┴────────────────────────────┘\n",
      "brand_country False\n",
      "shape: (7, 5)\n",
      "┌────────────────────┬───────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ brand_country_skey ┆ brand_country ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---                ┆ ---           ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64                ┆ str           ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞════════════════════╪═══════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1                  ┆ Us            ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.225710 │\n",
      "│ 2                  ┆ Usa           ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.225710 │\n",
      "│ 3                  ┆ India         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.225710 │\n",
      "│ 4                  ┆ China         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.225710 │\n",
      "│ 5                  ┆ Unknown       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.225710 │\n",
      "│ 6                  ┆ Japan         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.225710 │\n",
      "│ 7                  ┆ Germany       ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.225710 │\n",
      "└────────────────────┴───────────────┴───────────┴──────────┴────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "table_name = 'product'\n",
    "# key_col = ['gender', 'marital_status', 'customer_type', 'account_status']\n",
    "key_cols = {\"brand_tier\": False, \"brand_name\": False, \"brand_country\": False}\n",
    "\n",
    "for col, keep_unknown in key_cols.items():\n",
    "    print(col, keep_unknown)\n",
    "    src_file_name = staging_dir / f\"staging_{table_name}.parquet\"\n",
    "\n",
    "    df_src = pl.read_parquet(src_file_name)\n",
    "\n",
    "    # load gold table if exists\n",
    "    tgt_file_name = gold_dir / f\"gold_dim_{col}.parquet\"\n",
    "\n",
    "    if os.path.exists(tgt_file_name):\n",
    "        df_tgt = pl.read_parquet(tgt_file_name)\n",
    "    else:\n",
    "        df_tgt = None\n",
    "\n",
    "    # ##### TEST FILTER #####\n",
    "    # if col == 'gender':\n",
    "    #     df_src = df_src.filter(pl.col(col) != 'Female')\n",
    "    #     print(df_src['gender'].unique())\n",
    "    # if col == 'region':\n",
    "    #     df_src = df_src.filter(pl.col(col) != 'Africa')\n",
    "    #     print(df_src['region'].unique())\n",
    "\n",
    "    # merge\n",
    "    df_tgt = merge_upsert_dim(df_src, df_tgt, col, batch_id=1, keep_unknown=keep_unknown)\n",
    "    print(df_tgt)    \n",
    "\n",
    "    # save\n",
    "    df_tgt.write_parquet(tgt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "71a3fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "payment_source False\n",
      "shape: (7, 5)\n",
      "┌─────────────────────┬────────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ payment_source_skey ┆ payment_source ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---                 ┆ ---            ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64                 ┆ str            ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞═════════════════════╪════════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1                   ┆ Debit Card     ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.248975 │\n",
      "│ 2                   ┆ Paypal         ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.248975 │\n",
      "│ 3                   ┆ Net Banking    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.248975 │\n",
      "│ 4                   ┆ Upi            ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.248975 │\n",
      "│ 5                   ┆ Credit Card    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.248975 │\n",
      "│ 6                   ┆ Unknown        ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.248975 │\n",
      "│ 7                   ┆ Cash           ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.248975 │\n",
      "└─────────────────────┴────────────────┴───────────┴──────────┴────────────────────────────┘\n",
      "lead_type False\n",
      "shape: (4, 5)\n",
      "┌────────────────┬────────────┬───────────┬──────────┬────────────────────────────┐\n",
      "│ lead_type_skey ┆ lead_type  ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
      "│ ---            ┆ ---        ┆ ---       ┆ ---      ┆ ---                        │\n",
      "│ i64            ┆ str        ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
      "╞════════════════╪════════════╪═══════════╪══════════╪════════════════════════════╡\n",
      "│ 1              ┆ Retail     ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.253374 │\n",
      "│ 2              ┆ Unknown    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.253374 │\n",
      "│ 3              ┆ Website    ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.253374 │\n",
      "│ 4              ┆ Mobile App ┆ 1         ┆ 1        ┆ 2025-09-02 16:12:51.253374 │\n",
      "└────────────────┴────────────┴───────────┴──────────┴────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "table_name = 'order'\n",
    "# key_col = ['gender', 'marital_status', 'customer_type', 'account_status']\n",
    "key_cols = {\"payment_source\": False, \"lead_type\": False}\n",
    "\n",
    "for col, keep_unknown in key_cols.items():\n",
    "    print(col, keep_unknown)\n",
    "    src_file_name = staging_dir / f\"staging_{table_name}.parquet\"\n",
    "\n",
    "    df_src = pl.read_parquet(src_file_name)\n",
    "\n",
    "    # load gold table if exists\n",
    "    tgt_file_name = gold_dir / f\"gold_dim_{col}.parquet\"\n",
    "\n",
    "    if os.path.exists(tgt_file_name):\n",
    "        df_tgt = pl.read_parquet(tgt_file_name)\n",
    "    else:\n",
    "        df_tgt = None\n",
    "\n",
    "    # ##### TEST FILTER #####\n",
    "    # if col == 'gender':\n",
    "    #     df_src = df_src.filter(pl.col(col) != 'Female')\n",
    "    #     print(df_src['gender'].unique())\n",
    "    # if col == 'region':\n",
    "    #     df_src = df_src.filter(pl.col(col) != 'Africa')\n",
    "    #     print(df_src['region'].unique())\n",
    "\n",
    "    # merge\n",
    "    df_tgt = merge_upsert_dim(df_src, df_tgt, col, batch_id=1, keep_unknown=keep_unknown)\n",
    "    print(df_tgt)    \n",
    "\n",
    "    # save\n",
    "    df_tgt.write_parquet(tgt_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "197108cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "# def merge_scd2_dim_brand(df_src: pl.DataFrame, df_tgt: pl.DataFrame | None, batch_id: int) -> pl.DataFrame:\n",
    "def merge_scd2_dim(\n",
    "    df_src: pl.DataFrame,\n",
    "    df_tgt: pl.DataFrame | None,\n",
    "    key_col: str,                # e.g. \"brand_skey\"\n",
    "    attr_cols: list[str],        # e.g. [\"brand_tier_skey\", \"brand_name_skey\", \"brand_country_skey\"]\n",
    "    batch_id: int\n",
    ") -> pl.DataFrame:\n",
    "    \"\"\"\n",
    "    SCD-2 Merge for dim_brand using Polars.\n",
    "    \n",
    "    Rules:\n",
    "      - If brand_skey exists in both and attributes match -> keep as is\n",
    "      - If brand_skey exists only in source -> insert as new row\n",
    "      - If brand_skey exists only in target -> expire old row\n",
    "      - If brand_skey exists in both but attributes differ -> expire old + insert new version\n",
    "    \"\"\"\n",
    "\n",
    "    today = datetime.now().date()\n",
    "\n",
    "    if df_tgt is None or df_tgt.is_empty():\n",
    "        # first load → insert all rows as current\n",
    "        return (\n",
    "            df_src\n",
    "            .with_columns([\n",
    "                pl.lit(1).alias(\"is_active\"),\n",
    "                pl.lit(today).alias(\"start_date\"),\n",
    "                pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "                pl.lit(1).alias(\"is_current\"),\n",
    "                pl.lit(batch_id).alias(\"batch_id\"),\n",
    "                pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "            ])\n",
    "        )\n",
    "    # print(\"\\n============\\ndf_tgt : \", df_tgt)\n",
    "    # -------------------\n",
    "    # 1. Expire rows where attributes changed or missing in source\n",
    "    # -------------------\n",
    "    tgt_active = df_tgt.filter(pl.col(\"is_current\") == 1)\n",
    "    # print(\"\\n============\\ntgt_active : \", tgt_active)\n",
    "\n",
    "    # Join source with active target\n",
    "    joined = tgt_active.join(\n",
    "        df_src,\n",
    "        on=\"brand_skey\",\n",
    "        how=\"full\",\n",
    "        suffix=\"_src\"\n",
    "    )\n",
    "    # print(\"\\n============\\njoined : \", joined)\n",
    "\n",
    "    # Case A: Row exists in both but attributes differ → expire old\n",
    "    # expire_changed = joined.filter(\n",
    "    #     (pl.col(\"brand_skey\").is_not_null()) & (\n",
    "    #         (pl.col(\"brand_tier_skey\") != pl.col(\"brand_tier_skey_src\")) |\n",
    "    #         (pl.col(\"brand_name_skey\") != pl.col(\"brand_name_skey_src\")) |\n",
    "    #         (pl.col(\"brand_country_skey\") != pl.col(\"brand_country_skey_src\"))\n",
    "    #     )\n",
    "    # ).select(\"brand_skey\")\n",
    "    cond_changed = None\n",
    "    for c in attr_cols:\n",
    "        diff = pl.col(c) != pl.col(f\"{c}_src\")\n",
    "        cond_changed = diff if cond_changed is None else (cond_changed | diff)\n",
    "\n",
    "    expire_changed = joined.filter(\n",
    "        (pl.col(key_col).is_not_null()) & cond_changed\n",
    "    ).select(key_col)\n",
    "    # print(\"\\n============\\nexpire_changed : \", expire_changed)\n",
    "\n",
    "    # Case B: Row exists in target but missing in source → expire old\n",
    "    expire_missing = joined.filter(pl.col(\"brand_tier_skey_src\").is_null()).select(\"brand_skey\")\n",
    "    # print(\"\\n============\\nexpire_missing : \", expire_missing)\n",
    "\n",
    "    to_expire = pl.concat([expire_changed, expire_missing]).unique()\n",
    "    # print(\"\\n============\\nto_expire : \", to_expire)\n",
    "    \n",
    "    expire_keys = to_expire[\"brand_skey\"].to_list()\n",
    "    df_tgt = df_tgt.with_columns(\n",
    "        # pl.when(pl.col(\"brand_skey\").is_in(to_expire[\"brand_skey\"]))\n",
    "        #   .then(0).otherwise(pl.col(\"is_active\")).alias(\"is_active\"),\n",
    "        # pl.when(pl.col(\"brand_skey\").is_in(to_expire[\"brand_skey\"]))\n",
    "        #   .then(0).otherwise(pl.col(\"is_current\")).alias(\"is_current\"),\n",
    "        # pl.when(pl.col(\"brand_skey\").is_in(to_expire[\"brand_skey\"]))\n",
    "        #   .then(today).otherwise(pl.col(\"end_date\")).alias(\"end_date\")\n",
    "        \n",
    "        pl.when(pl.col(\"brand_skey\").is_in(expire_keys))\n",
    "            .then(0).otherwise(pl.col(\"is_active\")).alias(\"is_active\"),\n",
    "        pl.when(pl.col(\"brand_skey\").is_in(expire_keys))\n",
    "            .then(0).otherwise(pl.col(\"is_current\")).alias(\"is_current\"),\n",
    "        pl.when(pl.col(\"brand_skey\").is_in(expire_keys))\n",
    "            .then(today).otherwise(pl.col(\"end_date\")).alias(\"end_date\")\n",
    "    )\n",
    "    # print(\"\\n============\\ndf_tgt : \", df_tgt)\n",
    "    # print(\"\\n============\\ndf_tgt : \", df_tgt.to_pandas())\n",
    "\n",
    "    # -------------------\n",
    "    # 2. Insert new versions for changed or new keys\n",
    "    # -------------------\n",
    "    # Case C: Brand-new rows\n",
    "    # new_rows = df_src.filter(~pl.col(\"brand_skey\").is_in(tgt_active[\"brand_skey\"]))\n",
    "    new_rows = df_src.filter(~pl.col(\"brand_skey\").is_in(tgt_active[\"brand_skey\"].to_list()))\n",
    "    # print(\"\\n============\\nnew_rows : \", new_rows)\n",
    "\n",
    "    # Case D: New version for changed rows\n",
    "    # changed_rows = df_src.filter(pl.col(\"brand_skey\").is_in(expire_changed[\"brand_skey\"]))\n",
    "    changed_rows = df_src.filter(pl.col(\"brand_skey\").is_in(expire_changed[\"brand_skey\"].to_list()))\n",
    "    # print(\"\\n============\\nchanged_rows : \", changed_rows)\n",
    "\n",
    "    inserts = pl.concat([new_rows, changed_rows]).with_columns([\n",
    "        pl.lit(today).alias(\"start_date\"),\n",
    "        pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "        pl.lit(1).alias(\"is_current\"),\n",
    "        pl.lit(1).alias(\"is_active\"),\n",
    "        pl.lit(batch_id).alias(\"batch_id\"),\n",
    "        pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "    ]).sort(\"brand_skey\")\n",
    "    # print(\"\\n============\\ninserts : \", inserts)\n",
    "\n",
    "    # print(\"\\n============\\n df_tgt : \", df_tgt.columns)\n",
    "    # print(\"\\n============\\n inserts : \", inserts.columns)\n",
    "    \n",
    "    # align schemas to avoid Int32/Int64 mismatch\n",
    "    common_schema = {col: pl.Int64 for col, dtype in zip(df_tgt.columns, df_tgt.dtypes) if dtype in (pl.Int32, pl.Int64)}\n",
    "\n",
    "    df_tgt = df_tgt.cast(common_schema)\n",
    "    inserts = inserts.cast(common_schema)\n",
    "\n",
    "    # -------------------\n",
    "    # 3. Final result\n",
    "    # -------------------\n",
    "    df_final = pl.concat([df_tgt, inserts], how=\"vertical\").sort(\"brand_skey\")\n",
    "    # print(\"\\n============\\ndf_final : \", df_final)\n",
    "\n",
    "    return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "68486aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_name = \"product\"\n",
    "key_col = \"brand_skey\"\n",
    "attr_cols = [\"brand_tier_skey\", \"brand_name_skey\", \"brand_country_skey\"]\n",
    "\n",
    "src_file_name = staging_dir / f\"staging_{table_name}.parquet\"\n",
    "df_src = pl.read_parquet(src_file_name)\n",
    "\n",
    "tgt_file_name = gold_dir / f\"gold_dim_{table_name}.parquet\"\n",
    "df_tgt = pl.read_parquet(tgt_file_name) if os.path.exists(tgt_file_name) else None\n",
    "\n",
    "df_tgt = merge_scd2_dim(df_src, df_tgt, key_col, attr_cols, batch_id=1)\n",
    "\n",
    "df_tgt.write_parquet(tgt_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036012ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dim_parquet = pl.read_parquet(f\"gold_dim_{table_name}.parquet\")\n",
    "df_dim_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2b74c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import polars as pl\n",
    "# from datetime import datetime\n",
    "\n",
    "# def merge_scd2_dim_brand(df_src: pl.DataFrame, df_tgt: pl.DataFrame | None, batch_id: int) -> pl.DataFrame:\n",
    "#     \"\"\"\n",
    "#     SCD-2 Merge for dim_brand using Polars.\n",
    "    \n",
    "#     Rules:\n",
    "#       - If brand_skey exists in both and attributes match -> keep as is\n",
    "#       - If brand_skey exists only in source -> insert as new row\n",
    "#       - If brand_skey exists only in target -> expire old row\n",
    "#       - If brand_skey exists in both but attributes differ -> expire old + insert new version\n",
    "#     \"\"\"\n",
    "\n",
    "#     today = datetime.now().date()\n",
    "\n",
    "#     if df_tgt is None or df_tgt.is_empty():\n",
    "#         # first load → insert all rows as current\n",
    "#         return (\n",
    "#             df_src\n",
    "#             .with_columns([\n",
    "#                 pl.lit(1).alias(\"is_active\"),\n",
    "#                 pl.lit(today).alias(\"start_date\"),\n",
    "#                 pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "#                 pl.lit(1).alias(\"is_current\"),\n",
    "#                 pl.lit(batch_id).alias(\"batch_id\"),\n",
    "#                 pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "#             ])\n",
    "#         )\n",
    "#     # print(\"\\n============\\ndf_tgt : \", df_tgt)\n",
    "#     # -------------------\n",
    "#     # 1. Expire rows where attributes changed or missing in source\n",
    "#     # -------------------\n",
    "#     tgt_active = df_tgt.filter(pl.col(\"is_current\") == 1)\n",
    "#     # print(\"\\n============\\ntgt_active : \", tgt_active)\n",
    "\n",
    "#     # Join source with active target\n",
    "#     joined = tgt_active.join(\n",
    "#         df_src,\n",
    "#         on=\"brand_skey\",\n",
    "#         how=\"full\",\n",
    "#         suffix=\"_src\"\n",
    "#     )\n",
    "#     # print(\"\\n============\\njoined : \", joined)\n",
    "\n",
    "#     # Case A: Row exists in both but attributes differ → expire old\n",
    "#     expire_changed = joined.filter(\n",
    "#         (pl.col(\"brand_skey\").is_not_null()) & (\n",
    "#             (pl.col(\"brand_tier_skey\") != pl.col(\"brand_tier_skey_src\")) |\n",
    "#             (pl.col(\"brand_name_skey\") != pl.col(\"brand_name_skey_src\")) |\n",
    "#             (pl.col(\"brand_country_skey\") != pl.col(\"brand_country_skey_src\"))\n",
    "#         )\n",
    "#     ).select(\"brand_skey\")\n",
    "#     # print(\"\\n============\\nexpire_changed : \", expire_changed)\n",
    "\n",
    "#     # Case B: Row exists in target but missing in source → expire old\n",
    "#     expire_missing = joined.filter(pl.col(\"brand_tier_skey_src\").is_null()).select(\"brand_skey\")\n",
    "#     # print(\"\\n============\\nexpire_missing : \", expire_missing)\n",
    "\n",
    "#     to_expire = pl.concat([expire_changed, expire_missing]).unique()\n",
    "#     # print(\"\\n============\\nto_expire : \", to_expire)\n",
    "    \n",
    "#     expire_keys = to_expire[\"brand_skey\"].to_list()\n",
    "#     df_tgt = df_tgt.with_columns(\n",
    "#         # pl.when(pl.col(\"brand_skey\").is_in(to_expire[\"brand_skey\"]))\n",
    "#         #   .then(0).otherwise(pl.col(\"is_active\")).alias(\"is_active\"),\n",
    "#         # pl.when(pl.col(\"brand_skey\").is_in(to_expire[\"brand_skey\"]))\n",
    "#         #   .then(0).otherwise(pl.col(\"is_current\")).alias(\"is_current\"),\n",
    "#         # pl.when(pl.col(\"brand_skey\").is_in(to_expire[\"brand_skey\"]))\n",
    "#         #   .then(today).otherwise(pl.col(\"end_date\")).alias(\"end_date\")\n",
    "        \n",
    "#         pl.when(pl.col(\"brand_skey\").is_in(expire_keys))\n",
    "#             .then(0).otherwise(pl.col(\"is_active\")).alias(\"is_active\"),\n",
    "#         pl.when(pl.col(\"brand_skey\").is_in(expire_keys))\n",
    "#             .then(0).otherwise(pl.col(\"is_current\")).alias(\"is_current\"),\n",
    "#         pl.when(pl.col(\"brand_skey\").is_in(expire_keys))\n",
    "#             .then(today).otherwise(pl.col(\"end_date\")).alias(\"end_date\")\n",
    "#     )\n",
    "#     # print(\"\\n============\\ndf_tgt : \", df_tgt)\n",
    "#     # print(\"\\n============\\ndf_tgt : \", df_tgt.to_pandas())\n",
    "\n",
    "#     # -------------------\n",
    "#     # 2. Insert new versions for changed or new keys\n",
    "#     # -------------------\n",
    "#     # Case C: Brand-new rows\n",
    "#     # new_rows = df_src.filter(~pl.col(\"brand_skey\").is_in(tgt_active[\"brand_skey\"]))\n",
    "#     new_rows = df_src.filter(~pl.col(\"brand_skey\").is_in(tgt_active[\"brand_skey\"].to_list()))\n",
    "#     # print(\"\\n============\\nnew_rows : \", new_rows)\n",
    "\n",
    "#     # Case D: New version for changed rows\n",
    "#     # changed_rows = df_src.filter(pl.col(\"brand_skey\").is_in(expire_changed[\"brand_skey\"]))\n",
    "#     changed_rows = df_src.filter(pl.col(\"brand_skey\").is_in(expire_changed[\"brand_skey\"].to_list()))\n",
    "#     # print(\"\\n============\\nchanged_rows : \", changed_rows)\n",
    "\n",
    "#     inserts = pl.concat([new_rows, changed_rows]).with_columns([\n",
    "#         pl.lit(today).alias(\"start_date\"),\n",
    "#         pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "#         pl.lit(1).alias(\"is_current\"),\n",
    "#         pl.lit(1).alias(\"is_active\"),\n",
    "#         pl.lit(batch_id).alias(\"batch_id\"),\n",
    "#         pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "#     ]).sort(\"brand_skey\")\n",
    "#     # print(\"\\n============\\ninserts : \", inserts)\n",
    "\n",
    "#     # print(\"\\n============\\n df_tgt : \", df_tgt.columns)\n",
    "#     # print(\"\\n============\\n inserts : \", inserts.columns)\n",
    "    \n",
    "#     # align schemas to avoid Int32/Int64 mismatch\n",
    "#     common_schema = {col: pl.Int64 for col, dtype in zip(df_tgt.columns, df_tgt.dtypes) if dtype in (pl.Int32, pl.Int64)}\n",
    "\n",
    "#     df_tgt = df_tgt.cast(common_schema)\n",
    "#     inserts = inserts.cast(common_schema)\n",
    "\n",
    "#     # -------------------\n",
    "#     # 3. Final result\n",
    "#     # -------------------\n",
    "#     df_final = pl.concat([df_tgt, inserts], how=\"vertical\").sort(\"brand_skey\")\n",
    "#     # print(\"\\n============\\ndf_final : \", df_final)\n",
    "\n",
    "#     return df_final\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0240ce3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### --- TEST CASE ---\n",
    "# df_tgt = pl.DataFrame({\n",
    "#     \"brand_skey\": [1, 2, 3],\n",
    "#     \"brand_tier_skey\": [5, 6, 7],\n",
    "#     \"brand_name_skey\": [10, 20, 30],\n",
    "#     \"brand_country_skey\": [100, 200, 300],\n",
    "#     \"start_date\": [datetime(2023,1,1).date()]*3,\n",
    "#     \"end_date\": [None, None, None],\n",
    "#     \"is_current\": [1, 1, 1],\n",
    "#     \"is_active\": [1, 1, 1],\n",
    "#     \"batch_id\": [0, 0, 0],\n",
    "#     \"load_timestamp\": [datetime(2023,1,1)]*3,\n",
    "# })\n",
    "\n",
    "# df_src = pl.DataFrame({\n",
    "#     \"brand_skey\": [1, 3, 4],\n",
    "#     \"brand_tier_skey\": [5, 7, 8],\n",
    "#     \"brand_name_skey\": [10, 50, 40],\n",
    "#     \"brand_country_skey\": [100, 600, 400],\n",
    "# })\n",
    "\n",
    "# df_final = merge_scd2_dim_brand(df_src, df_tgt, batch_id=1)\n",
    "# print(df_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "5cf3acd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from datetime import datetime\n",
    "\n",
    "# ---------- 1. Root-level (simple UPSERT) ----------\n",
    "def merge_upsert_dim_chain(df_src, df_tgt, key_col, batch_id, keep_unknown=False, unknown_value=\"Unknown\"):\n",
    "    df_src = df_src.select(key_col).drop_nulls().unique()\n",
    "    if df_tgt is None or df_tgt.is_empty():\n",
    "        base = df_src if not keep_unknown else df_src.filter(pl.col(key_col) != unknown_value)\n",
    "        df_final = (\n",
    "            base\n",
    "            .with_row_index(f\"{key_col}_skey\", offset=1)\n",
    "            .with_columns([\n",
    "                pl.col(f\"{key_col}_skey\").cast(pl.Int64),\n",
    "                pl.lit(1).alias(\"is_active\"),\n",
    "                pl.lit(batch_id).alias(\"batch_id\"),\n",
    "                pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "            ])\n",
    "        )\n",
    "        if keep_unknown:\n",
    "            unknown_row = pl.DataFrame({\n",
    "                f\"{key_col}_skey\": [int(df_final[f\"{key_col}_skey\"].max()) + 1 if not df_final.is_empty() else 1],\n",
    "                key_col: [unknown_value],\n",
    "                \"is_active\": [1],\n",
    "                \"batch_id\": [batch_id],\n",
    "                \"load_timestamp\": [datetime.now()],\n",
    "            }).cast(df_final.schema)\n",
    "            df_final = pl.concat([df_final, unknown_row])\n",
    "        return df_final.sort(f\"{key_col}_skey\")\n",
    "\n",
    "    # --- incremental ---\n",
    "    # new_rows = df_src.filter(~pl.col(key_col).is_in(df_tgt[key_col]))\n",
    "    new_rows = df_src.filter(~pl.col(key_col).is_in(df_tgt[key_col].implode()))\n",
    "    if not new_rows.is_empty():\n",
    "        start_offset = int(df_tgt[f\"{key_col}_skey\"].max()) + 1\n",
    "        new_rows = (\n",
    "            new_rows\n",
    "            .with_row_index(f\"{key_col}_skey\", offset=start_offset)\n",
    "            .with_columns([\n",
    "                pl.col(f\"{key_col}_skey\").cast(pl.Int64),\n",
    "                pl.lit(1).alias(\"is_active\"),\n",
    "                pl.lit(batch_id).alias(\"batch_id\"),\n",
    "                pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "            ])\n",
    "        )\n",
    "    else:\n",
    "        new_rows = df_tgt.head(0)\n",
    "\n",
    "    # common = df_tgt.filter(pl.col(key_col).is_in(df_src[key_col]))\n",
    "    common = df_tgt.filter(pl.col(key_col).is_in(df_src[key_col].implode()))\n",
    "    return pl.concat([common, new_rows]).unique([key_col]).sort(f\"{key_col}_skey\")\n",
    "\n",
    "\n",
    "# ---------- 2. Dependent levels (SCD2) ----------\n",
    "def merge_scd2_dim_chain(df_src, df_tgt, key_col, attr_cols, batch_id):\n",
    "    today = datetime.now().date()\n",
    "\n",
    "    # if df_tgt is None or df_tgt.is_empty():\n",
    "    #     return (\n",
    "    #         df_src\n",
    "    #         .with_columns([\n",
    "    #             pl.lit(today).alias(\"start_date\"),\n",
    "    #             pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "    #             pl.lit(1).alias(\"is_current\"),\n",
    "    #             pl.lit(1).alias(\"is_active\"),\n",
    "    #             pl.lit(batch_id).alias(\"batch_id\"),\n",
    "    #             pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "    #         ])\n",
    "    #     )\n",
    "    if df_tgt is None or df_tgt.is_empty():\n",
    "        return (\n",
    "            df_src\n",
    "            .with_row_index(f\"{key_col}_skey\", offset=1)\n",
    "            .with_columns([\n",
    "                pl.col(f\"{key_col}_skey\").cast(pl.Int64),\n",
    "                pl.lit(today).alias(\"start_date\"),\n",
    "                pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "                pl.lit(1).alias(\"is_current\"),\n",
    "                pl.lit(1).alias(\"is_active\"),\n",
    "                pl.lit(batch_id).alias(\"batch_id\"),\n",
    "                pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "    tgt_active = df_tgt.filter(pl.col(\"is_current\") == 1)\n",
    "    joined = tgt_active.join(df_src, on=key_col, how=\"full\", suffix=\"_src\")\n",
    "\n",
    "    # --- detect changed rows ---\n",
    "    cond_changed = None\n",
    "    for c in attr_cols:\n",
    "        diff = pl.col(c) != pl.col(f\"{c}_src\")\n",
    "        cond_changed = diff if cond_changed is None else (cond_changed | diff)\n",
    "\n",
    "    expire_changed = joined.filter((pl.col(key_col).is_not_null()) & cond_changed).select(key_col)\n",
    "    expire_missing = joined.filter(pl.col(f\"{attr_cols[0]}_src\").is_null()).select(key_col)\n",
    "    expire_keys = pl.concat([expire_changed, expire_missing]).unique()[key_col].to_list()\n",
    "\n",
    "    df_tgt = df_tgt.with_columns([\n",
    "        pl.when(pl.col(key_col).is_in(expire_keys)).then(0).otherwise(pl.col(\"is_active\")).alias(\"is_active\"),\n",
    "        pl.when(pl.col(key_col).is_in(expire_keys)).then(0).otherwise(pl.col(\"is_current\")).alias(\"is_current\"),\n",
    "        pl.when(pl.col(key_col).is_in(expire_keys)).then(today).otherwise(pl.col(\"end_date\")).alias(\"end_date\"),\n",
    "    ])\n",
    "\n",
    "    # new_rows = df_src.filter(~pl.col(key_col).is_in(tgt_active[key_col]))\n",
    "    new_rows = df_src.filter(~pl.col(key_col).is_in(tgt_active[key_col].implode()))\n",
    "    # changed_rows = df_src.filter(pl.col(key_col).is_in(expire_changed[key_col]))\n",
    "    changed_rows = df_src.filter(pl.col(key_col).is_in(expire_changed[key_col].implode()))\n",
    "    # inserts = pl.concat([new_rows, changed_rows]).with_columns([\n",
    "    #     pl.lit(today).alias(\"start_date\"),\n",
    "    #     pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "    #     pl.lit(1).alias(\"is_current\"),\n",
    "    #     pl.lit(1).alias(\"is_active\"),\n",
    "    #     pl.lit(batch_id).alias(\"batch_id\"),\n",
    "    #     pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "    # ])\n",
    "    if not pl.concat([new_rows, changed_rows]).is_empty():\n",
    "        start_offset = int(df_tgt[f\"{key_col}_skey\"].max()) + 1\n",
    "        inserts = (\n",
    "            pl.concat([new_rows, changed_rows])\n",
    "            .with_row_index(f\"{key_col}_skey\", offset=start_offset)\n",
    "            .with_columns([\n",
    "                pl.col(f\"{key_col}_skey\").cast(pl.Int64),\n",
    "                pl.lit(today).alias(\"start_date\"),\n",
    "                pl.lit(None).cast(pl.Date).alias(\"end_date\"),\n",
    "                pl.lit(1).alias(\"is_current\"),\n",
    "                pl.lit(1).alias(\"is_active\"),\n",
    "                pl.lit(batch_id).alias(\"batch_id\"),\n",
    "                pl.lit(datetime.now()).alias(\"load_timestamp\"),\n",
    "            ])\n",
    "        )\n",
    "    else:\n",
    "        inserts = df_tgt.head(0)\n",
    "\n",
    "    # return pl.concat([df_tgt, inserts]).sort(key_col)\n",
    "    df_final = pl.concat([df_tgt, inserts])\n",
    "\n",
    "    # Ensure surrogate key exists\n",
    "    if f\"{key_col}_skey\" not in df_final.columns:\n",
    "        df_final = (\n",
    "            df_final\n",
    "            .with_row_index(f\"{key_col}_skey\", offset=1)\n",
    "            .with_columns(pl.col(f\"{key_col}_skey\").cast(pl.Int64))\n",
    "        )\n",
    "\n",
    "    return df_final.sort(f\"{key_col}_skey\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3e3b8b38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing region...\n",
      "Processing country...\n",
      "Processing state...\n",
      "Processing city...\n",
      "Processing postal_code...\n"
     ]
    }
   ],
   "source": [
    "tbl = \"customer\"\n",
    "df_customer = pl.read_parquet(staging_dir / f\"staging_{tbl}.parquet\")\n",
    "\n",
    "hierarchy = [\n",
    "    (\"region\", \"region\", []),\n",
    "    (\"country\", \"country\", [\"region_skey\"]),\n",
    "    (\"state\", \"state\", [\"country_skey\"]),\n",
    "    (\"city\", \"city\", [\"state_skey\"]),\n",
    "    (\"postal_code\", \"postal_code\", [\"city_skey\"]),\n",
    "]\n",
    "\n",
    "parent_map = {\n",
    "    \"country\": (\"region\", \"region_skey\"),\n",
    "    \"state\": (\"country\", \"country_skey\"),\n",
    "    \"city\": (\"state\", \"state_skey\"),\n",
    "    \"postal_code\": (\"city\", \"city_skey\"),\n",
    "}\n",
    "\n",
    "for table_name, key_col, attr_cols in hierarchy:\n",
    "    print(f\"Processing {table_name}...\")\n",
    "\n",
    "    if not attr_cols:\n",
    "        # Root level: region\n",
    "        df_src = df_customer.select([key_col]).drop_nulls().unique()\n",
    "    else:\n",
    "        # Need to lookup parent surrogate key\n",
    "        parent_name, parent_skey = parent_map[table_name]\n",
    "\n",
    "        parent_file = gold_dir / f\"gold_dim_{parent_name}.parquet\"\n",
    "        df_parent = pl.read_parquet(parent_file)\n",
    "\n",
    "        df_src = (\n",
    "            df_customer\n",
    "            .select([key_col, parent_name])          # natural keys only\n",
    "            .drop_nulls()\n",
    "            .unique()\n",
    "            .join(df_parent.select([parent_name, parent_skey]), on=parent_name, how=\"left\")\n",
    "            .select([key_col, parent_skey])\n",
    "        )\n",
    "\n",
    "    # Load gold if exists\n",
    "    tgt_file_name = gold_dir / f\"gold_dim_{table_name}.parquet\"\n",
    "    df_tgt = pl.read_parquet(tgt_file_name) if os.path.exists(tgt_file_name) else None\n",
    "\n",
    "    # Merge\n",
    "    if not attr_cols:\n",
    "        df_tgt = merge_upsert_dim(df_src, df_tgt, key_col, batch_id=1)\n",
    "    else:\n",
    "        df_tgt = merge_scd2_dim_chain(df_src, df_tgt, key_col, attr_cols, batch_id=1)\n",
    "\n",
    "    df_tgt.write_parquet(tgt_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "57861dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr,\n",
       ".dataframe > tbody > tr {\n",
       "  text-align: right;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 5)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>region_skey</th><th>region</th><th>is_active</th><th>batch_id</th><th>load_timestamp</th></tr><tr><td>i64</td><td>str</td><td>i32</td><td>i32</td><td>datetime[μs]</td></tr></thead><tbody><tr><td>1</td><td>&quot;South America&quot;</td><td>1</td><td>1</td><td>2025-09-03 14:35:03.756456</td></tr><tr><td>2</td><td>&quot;Asia&quot;</td><td>1</td><td>1</td><td>2025-09-03 14:35:03.756456</td></tr><tr><td>3</td><td>&quot;Europe&quot;</td><td>1</td><td>1</td><td>2025-09-03 14:35:03.756456</td></tr><tr><td>4</td><td>&quot;North America&quot;</td><td>1</td><td>1</td><td>2025-09-03 14:35:03.756456</td></tr><tr><td>5</td><td>&quot;Africa&quot;</td><td>1</td><td>1</td><td>2025-09-03 14:35:03.756456</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 5)\n",
       "┌─────────────┬───────────────┬───────────┬──────────┬────────────────────────────┐\n",
       "│ region_skey ┆ region        ┆ is_active ┆ batch_id ┆ load_timestamp             │\n",
       "│ ---         ┆ ---           ┆ ---       ┆ ---      ┆ ---                        │\n",
       "│ i64         ┆ str           ┆ i32       ┆ i32      ┆ datetime[μs]               │\n",
       "╞═════════════╪═══════════════╪═══════════╪══════════╪════════════════════════════╡\n",
       "│ 1           ┆ South America ┆ 1         ┆ 1        ┆ 2025-09-03 14:35:03.756456 │\n",
       "│ 2           ┆ Asia          ┆ 1         ┆ 1        ┆ 2025-09-03 14:35:03.756456 │\n",
       "│ 3           ┆ Europe        ┆ 1         ┆ 1        ┆ 2025-09-03 14:35:03.756456 │\n",
       "│ 4           ┆ North America ┆ 1         ┆ 1        ┆ 2025-09-03 14:35:03.756456 │\n",
       "│ 5           ┆ Africa        ┆ 1         ┆ 1        ┆ 2025-09-03 14:35:03.756456 │\n",
       "└─────────────┴───────────────┴───────────┴──────────┴────────────────────────────┘"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_name = 'region'\n",
    "df_dim_parquet = pl.read_parquet(f\"gold_dim_{table_name}.parquet\")\n",
    "df_dim_parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "3c32f727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### --- TEST CASE ---\n",
    "# # 1. Region\n",
    "# df_region_src = pl.DataFrame({\"region\": [\"Asia\", \"Europe\"]})\n",
    "# df_region_tgt = None\n",
    "# df_region_final = merge_upsert_dim_chain(df_region_src, df_region_tgt, \"region\", batch_id=1)\n",
    "\n",
    "# # 2. Country (depends on region)\n",
    "# df_country_src = pl.DataFrame({\"country\": [\"India\", \"Germany\"], \"region_skey\": [1, 2]})\n",
    "# df_country_tgt = None\n",
    "# df_country_final = merge_scd2_dim_chain(df_country_src, df_country_tgt, \"country\", [\"region_skey\"], batch_id=1)\n",
    "\n",
    "# # 3. State (depends on country)\n",
    "# df_state_src = pl.DataFrame({\"state\": [\"Karnataka\", \"Bavaria\"], \"country_skey\": [1, 2]})\n",
    "# df_state_tgt = None\n",
    "# df_state_final = merge_scd2_dim_chain(df_state_src, df_state_tgt, \"state\", [\"country_skey\"], batch_id=1)\n",
    "\n",
    "# print(df_region_src)\n",
    "# print(df_region_final)\n",
    "# print(df_country_src)\n",
    "# print(df_country_final)\n",
    "# print(df_state_src)\n",
    "# print(df_state_final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "08a2f31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ### --- TEST CASE ---\n",
    "# # ✅ new batch with a change: \"Europe\" → \"EU\"\n",
    "# # First load (initial batch)\n",
    "# df_region_final = merge_upsert_dim_chain(\n",
    "#     pl.DataFrame({\"region\": [\"Asia\", \"Europe\"]}),\n",
    "#     None,   # no previous tgt\n",
    "#     \"region\",\n",
    "#     batch_id=1\n",
    "# )\n",
    "\n",
    "# # Second load (update batch)\n",
    "# df_region_src_new = pl.DataFrame({\"region\": [\"Asia\", \"EU\"]})\n",
    "# df_region_final_updated = merge_upsert_dim_chain(\n",
    "#     df_region_src_new,\n",
    "#     df_region_final,   # use previous tgt (with region_skey)\n",
    "#     \"region\",\n",
    "#     batch_id=2\n",
    "# )\n",
    "\n",
    "# print(\"\\n=================\\n\", df_region_src_new)\n",
    "# print(df_region_final)\n",
    "# print(df_region_final_updated)\n",
    "\n",
    "# # ✅ Suppose India’s region_skey changed from 1→3 (new region mapping)\n",
    "# df_country_src_new = pl.DataFrame({\"country\": [\"India\", \"Germany\"], \"region_skey\": [3, 2]})\n",
    "# # Initial load for country\n",
    "# df_country_final = merge_scd2_dim_chain(\n",
    "#     pl.DataFrame({\"country\": [\"India\", \"Germany\"], \"region_skey\": [1, 2]}),\n",
    "#     None,\n",
    "#     \"country\",\n",
    "#     [\"region_skey\"],\n",
    "#     batch_id=1\n",
    "# )\n",
    "\n",
    "# # Next batch (update)\n",
    "# df_country_final_updated = merge_scd2_dim_chain(\n",
    "#     df_country_src_new,\n",
    "#     df_country_final,\n",
    "#     \"country\",\n",
    "#     [\"region_skey\"],\n",
    "#     batch_id=2\n",
    "# )\n",
    "\n",
    "\n",
    "# print(\"\\n=================\\n\", df_country_src_new)\n",
    "# print(df_country_final)\n",
    "# print(df_country_final_updated)\n",
    "\n",
    "# # ✅ Rename Karnataka → Bengaluru\n",
    "# df_state_src_new = pl.DataFrame({\"state\": [\"Bengaluru\", \"Bavaria\"], \"country_skey\": [1, 2]})\n",
    "# # Initial load for state\n",
    "# df_state_final = merge_scd2_dim_chain(\n",
    "#     pl.DataFrame({\"state\": [\"Karnataka\", \"Bavaria\"], \"country_skey\": [1, 2]}),\n",
    "#     None,\n",
    "#     \"state\",\n",
    "#     [\"country_skey\"],\n",
    "#     batch_id=1\n",
    "# )\n",
    "\n",
    "# # Next batch (update)\n",
    "# df_state_final_updated = merge_scd2_dim_chain(\n",
    "#     df_state_src_new,\n",
    "#     df_state_final,   # previous tgt\n",
    "#     \"state\",\n",
    "#     [\"country_skey\"],\n",
    "#     batch_id=2\n",
    "# )\n",
    "\n",
    "# print(\"\\n=================\\n\", df_state_src_new)\n",
    "# print(df_state_final)\n",
    "# print(df_state_final_updated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62080727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a9bcd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0e0b9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
